{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8f88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14dc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria√ß√£o da SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExemploSparkSession\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14cf0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = r\"C:\\Users\\fred\\meu_projeto_etl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac608565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa√ß√£o da tabela base\n",
    "train_base = spark.read.parquet(fr\"{caminho}\\\\data\\interim\\train_base_tratada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e2b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_applprev_1 = spark.read.parquet(fr\"{caminho}\\\\data\\interim\\train_applprev_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd4c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bureau_a_1 = spark.read.parquet(fr\"{caminho}\\\\data\\interim\\train_bureau_a_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54373b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bureau_a_2 = spark.read.parquet(fr\"{caminho}\\\\data\\interim\\train_bureau_a_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a657019",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bureau_b_1 = spark.read.parquet(fr\"{caminho}\\\\data\\interim\\train_bureau_b_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cc6c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, TimestampType\n",
    "from functools import reduce  # ‚¨ÖÔ∏è Importar do m√≥dulo padr√£o\n",
    "\n",
    "def filtrar_temporal_e_auditar(df, anchors, nome_dataset=None):\n",
    "    if nome_dataset is None:\n",
    "        nome_dataset = \"dataset\"\n",
    "\n",
    "    # Identifica colunas de data no schema do DataFrame Spark\n",
    "    datetime_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, (DateType, TimestampType))]\n",
    "\n",
    "    if not datetime_cols:\n",
    "        print(f\"‚ö†Ô∏è Nenhuma coluna de data encontrada no DataFrame: {nome_dataset}.\")\n",
    "        return df, []\n",
    "\n",
    "    # Prepara anchors: garantir que est√° com tipo de data\n",
    "    anchors = anchors.withColumn(\"decision_date\", F.to_timestamp(\"date_decision\"))\n",
    "\n",
    "    # Junta com anchors\n",
    "    df_merged = df.join(anchors, on=\"case_id\", how=\"left\")\n",
    "\n",
    "    # Filtro: mant√©m apenas linhas em que TODAS as colunas datetime <= decision_date\n",
    "    condicoes = [(F.col(col).isNull()) | (F.col(col) <= F.col(\"decision_date\")) for col in datetime_cols]\n",
    "    condicao_final = reduce(lambda a, b: a & b, condicoes)\n",
    "\n",
    "    df_filtrado = df_merged.filter(condicao_final).drop(\"decision_date\")\n",
    "\n",
    "    # Contagem para relat√≥rio\n",
    "    total_antes = df.count()\n",
    "    total_depois = df_filtrado.count()\n",
    "    removidos = total_antes - total_depois\n",
    "\n",
    "    print(f\"üìä Colunas de data consideradas para `{nome_dataset}`: {datetime_cols}\")\n",
    "    print(f\"üìä Filtro aplicado: {removidos} registros removidos por datas > date_decision\")\n",
    "\n",
    "    return df_filtrado, datetime_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43310a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Colunas de data consideradas para `train_applprev_1`: ['approvaldate_319D', 'creationdate_885D', 'dateactivated_425D', 'dtlastpmt_581D', 'dtlastpmtallstes_3545839D', 'employedfrom_700D', 'firstnonzeroinstldate_307D']\n",
      "üìä Filtro aplicado: 582274 registros removidos por datas > date_decision\n",
      "üìä Colunas de data consideradas para `train_bureau_a_1`: ['dateofcredend_289D', 'dateofcredend_353D', 'dateofcredstart_181D', 'dateofcredstart_739D', 'dateofrealrepmt_138D', 'lastupdate_1112D', 'lastupdate_388D', 'numberofoverdueinstlmaxdat_148D', 'numberofoverdueinstlmaxdat_641D', 'overdueamountmax2date_1002D', 'overdueamountmax2date_1142D', 'refreshdate_3813885D']\n",
      "üìä Filtro aplicado: 6751470 registros removidos por datas > date_decision\n",
      "‚ö†Ô∏è Nenhuma coluna de data encontrada no DataFrame: train_bureau_a_2.\n",
      "üìä Colunas de data consideradas para `train_bureau_b_1`: ['contractdate_551D', 'contractmaturitydate_151D', 'lastupdate_260D']\n",
      "üìä Filtro aplicado: 80907 registros removidos por datas > date_decision\n"
     ]
    }
   ],
   "source": [
    "anchors = train_base.select(\"case_id\", \"date_decision\")\n",
    "\n",
    "applprev1_filtrado, colunas_data = filtrar_temporal_e_auditar(train_applprev_1, anchors, \"train_applprev_1\")\n",
    "bureau_a1_filtrado, colunas_data = filtrar_temporal_e_auditar(train_bureau_a_1, anchors, \"train_bureau_a_1\")\n",
    "bureau_a2_filtrado, colunas_data = filtrar_temporal_e_auditar(train_bureau_a_2, anchors, \"train_bureau_a_2\")\n",
    "bureau_b1_filtrado, colunas_data = filtrar_temporal_e_auditar(train_bureau_b_1, anchors, \"train_bureau_b_1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "612bdb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9189067"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_a1_filtrado.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2758637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case_id',\n",
       " 'actualdpd_943P',\n",
       " 'annuity_853A',\n",
       " 'approvaldate_319D',\n",
       " 'byoccupationinc_3656910L',\n",
       " 'cancelreason_3545846M',\n",
       " 'childnum_21L',\n",
       " 'creationdate_885D',\n",
       " 'credacc_actualbalance_314A',\n",
       " 'credacc_credlmt_575A',\n",
       " 'credacc_maxhisbal_375A',\n",
       " 'credacc_minhisbal_90A',\n",
       " 'credacc_status_367L',\n",
       " 'credacc_transactions_402L',\n",
       " 'credamount_590A',\n",
       " 'credtype_587L',\n",
       " 'currdebt_94A',\n",
       " 'dateactivated_425D',\n",
       " 'district_544M',\n",
       " 'downpmt_134A',\n",
       " 'dtlastpmt_581D',\n",
       " 'dtlastpmtallstes_3545839D',\n",
       " 'education_1138M',\n",
       " 'employedfrom_700D',\n",
       " 'familystate_726L',\n",
       " 'firstnonzeroinstldate_307D',\n",
       " 'inittransactioncode_279L',\n",
       " 'isbidproduct_390L',\n",
       " 'isdebitcard_527L',\n",
       " 'mainoccupationinc_437A',\n",
       " 'maxdpdtolerance_577P',\n",
       " 'num_group1',\n",
       " 'outstandingdebt_522A',\n",
       " 'pmtnum_8L',\n",
       " 'postype_4733339M',\n",
       " 'profession_152M',\n",
       " 'rejectreason_755M',\n",
       " 'rejectreasonclient_4145042M',\n",
       " 'revolvingaccount_394A',\n",
       " 'status_219L',\n",
       " 'tenor_203L',\n",
       " 'dias_para_aprovacao',\n",
       " 'dias_ate_ativacao',\n",
       " 'dias_ult_pagamento',\n",
       " 'dias_ult_pagamento_all',\n",
       " 'dias_desde_inicio_emprego',\n",
       " 'dias_para_primeira_parcela',\n",
       " 'sem_aprovacao_flag',\n",
       " 'sem_ativacao_flag',\n",
       " 'sem_pagamento_flag',\n",
       " 'sem_pagamento_total_flag',\n",
       " 'sem_emprego_flag',\n",
       " 'sem_parcela_flag',\n",
       " 'mainoccupationinc_null_flag',\n",
       " 'byoccupationinc_null_flag',\n",
       " 'tem_revolving_flag',\n",
       " 'divida_total',\n",
       " 'limite_cartao_credito_flag',\n",
       " 'sem_historico_credito_flag']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_applprev_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226b4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b499e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "    \"annuity_853A\",\n",
    "    \"credacc_actualbalance_314A\",\n",
    "    \"credacc_credlmt_575A\",\n",
    "    \"credacc_maxhisbal_375A\",\n",
    "    \"credacc_minhisbal_90A\",\n",
    "    \"credacc_transactions_402L\",\n",
    "    \"credamount_590A\",\n",
    "    \"currdebt_94A\",\n",
    "    \"downpmt_134A\",\n",
    "    \"mainoccupationinc_437A\",\n",
    "    \"byoccupationinc_3656910L\",\n",
    "    \"outstandingdebt_522A\",\n",
    "    \"pmtnum_8L\",\n",
    "    \"revolvingaccount_394A\",\n",
    "    \"tenor_203L\",\n",
    "    \"divida_total\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "502d2805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Coluna annuity_853A ‚Üí tipo detectado: double\n",
      "üìä Coluna credacc_actualbalance_314A ‚Üí tipo detectado: double\n",
      "üìä Coluna credacc_credlmt_575A ‚Üí tipo detectado: double\n",
      "üìä Coluna credacc_maxhisbal_375A ‚Üí tipo detectado: double\n",
      "üìä Coluna credacc_minhisbal_90A ‚Üí tipo detectado: double\n",
      "üìä Coluna credacc_transactions_402L ‚Üí tipo detectado: int\n",
      "üìä Coluna credamount_590A ‚Üí tipo detectado: double\n",
      "üìä Coluna currdebt_94A ‚Üí tipo detectado: double\n",
      "üìä Coluna downpmt_134A ‚Üí tipo detectado: double\n",
      "üìä Coluna mainoccupationinc_437A ‚Üí tipo detectado: double\n",
      "üìä Coluna byoccupationinc_3656910L ‚Üí tipo detectado: double\n",
      "üìä Coluna outstandingdebt_522A ‚Üí tipo detectado: double\n",
      "üìä Coluna pmtnum_8L ‚Üí tipo detectado: int\n",
      "üìä Coluna revolvingaccount_394A ‚Üí tipo detectado: double\n",
      "üìä Coluna tenor_203L ‚Üí tipo detectado: int\n",
      "üìä Coluna divida_total ‚Üí tipo detectado: double\n",
      "‚úÖ Agrega√ß√£o num√©rica conclu√≠da.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Checar tipos reais no DataFrame\n",
    "schema = dict(applprev1_filtrado.dtypes)\n",
    "\n",
    "for c in num_cols:\n",
    "    tipo = schema.get(c)\n",
    "    print(f\"üìä Coluna {c} ‚Üí tipo detectado: {tipo}\")\n",
    "\n",
    "    if tipo not in [\"int\", \"bigint\", \"double\", \"float\", \"decimal\"]:\n",
    "        print(f\"‚ö†Ô∏è Coluna {c} n√£o √© num√©rica, ser√° ignorada.\")\n",
    "        num_cols.remove(c)\n",
    "\n",
    "# 2) Definir agrega√ß√µes padr√£o para num√©ricas\n",
    "aggs_num = [\n",
    "    F.sum(c).alias(f\"{c}_sum\") for c in num_cols\n",
    "] + [\n",
    "    F.avg(c).alias(f\"{c}_avg\") for c in num_cols\n",
    "] + [\n",
    "    F.max(c).alias(f\"{c}_max\") for c in num_cols\n",
    "] + [\n",
    "    F.min(c).alias(f\"{c}_min\") for c in num_cols\n",
    "]\n",
    "\n",
    "# 3) Executar agrega√ß√£o por case_id\n",
    "agg_applprev_1_num = applprev1_filtrado.groupBy(\"case_id\").agg(*aggs_num)\n",
    "\n",
    "print(\"‚úÖ Agrega√ß√£o num√©rica conclu√≠da.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "022ff99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Coluna approvaldate_319D ‚Üí tipo detectado: date\n",
      "üìÖ Coluna creationdate_885D ‚Üí tipo detectado: date\n",
      "üìÖ Coluna dateactivated_425D ‚Üí tipo detectado: date\n",
      "üìÖ Coluna dtlastpmt_581D ‚Üí tipo detectado: date\n",
      "üìÖ Coluna dtlastpmtallstes_3545839D ‚Üí tipo detectado: date\n",
      "üìÖ Coluna employedfrom_700D ‚Üí tipo detectado: date\n",
      "üìÖ Coluna firstnonzeroinstldate_307D ‚Üí tipo detectado: date\n",
      "‚úÖ Agrega√ß√£o de datas conclu√≠da.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Liste aqui as colunas de DATA do train_applprev_1\n",
    "date_cols = [\n",
    "    \"approvaldate_319D\",\n",
    "    \"creationdate_885D\",\n",
    "    \"dateactivated_425D\",\n",
    "    \"dtlastpmt_581D\",\n",
    "    \"dtlastpmtallstes_3545839D\",\n",
    "    \"employedfrom_700D\",\n",
    "    \"firstnonzeroinstldate_307D\",\n",
    "]\n",
    "\n",
    "# 2) Checar tipos no DataFrame e filtrar apenas date/timestamp\n",
    "schema = dict(applprev1_filtrado.dtypes)\n",
    "valid_date_cols = []\n",
    "for c in date_cols:\n",
    "    t = schema.get(c)\n",
    "    print(f\"üìÖ Coluna {c} ‚Üí tipo detectado: {t}\")\n",
    "    if t in (\"date\", \"timestamp\"):\n",
    "        valid_date_cols.append(c)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {c} n√£o est√° em 'date'/'timestamp' e ser√° ignorada na agrega√ß√£o.\")\n",
    "\n",
    "if not valid_date_cols:\n",
    "    print(\"‚ö†Ô∏è Nenhuma coluna de data v√°lida encontrada para agregar.\")\n",
    "else:\n",
    "    # 3) Agrega√ß√µes: m√≠nima (primeira ocorr√™ncia) e m√°xima (√∫ltima ocorr√™ncia)\n",
    "    aggs_dates = []\n",
    "    for c in valid_date_cols:\n",
    "        aggs_dates.append(F.min(c).alias(f\"{c}_min\"))\n",
    "        aggs_dates.append(F.max(c).alias(f\"{c}_max\"))\n",
    "\n",
    "    # 4) Executar agrega√ß√£o por case_id\n",
    "    agg_applprev_1_dates = applprev1_filtrado.groupBy(\"case_id\").agg(*aggs_dates)\n",
    "    print(\"‚úÖ Agrega√ß√£o de datas conclu√≠da.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "318eb108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agrega√ß√µes num√©ricas conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "num_cols = [\n",
    "    \"annuity_853A\",\"credacc_actualbalance_314A\",\"credacc_credlmt_575A\",\n",
    "    \"credacc_maxhisbal_375A\",\"credacc_minhisbal_90A\",\"credacc_transactions_402L\",\n",
    "    \"credamount_590A\",\"currdebt_94A\",\"downpmt_134A\",\"mainoccupationinc_437A\",\n",
    "    \"byoccupationinc_3656910L\",\"outstandingdebt_522A\",\"pmtnum_8L\",\n",
    "    \"revolvingaccount_394A\",\"tenor_203L\",\"divida_total\"\n",
    "]\n",
    "\n",
    "schema = dict(applprev1_filtrado.dtypes)\n",
    "num_valid = [c for c in num_cols if schema.get(c) in (\"int\",\"bigint\",\"double\",\"float\",\"decimal\")]\n",
    "\n",
    "aggs_num = (\n",
    "    [F.sum(c).alias(f\"{c}_sum\") for c in num_valid] +\n",
    "    [F.avg(c).alias(f\"{c}_avg\") for c in num_valid] +\n",
    "    [F.max(c).alias(f\"{c}_max\") for c in num_valid] +\n",
    "    [F.min(c).alias(f\"{c}_min\") for c in num_valid]\n",
    ")\n",
    "\n",
    "agg_applprev_1_num = applprev1_filtrado.groupBy(\"case_id\").agg(*aggs_num)\n",
    "print(\"‚úÖ Agrega√ß√µes num√©ricas conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3121a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agrega√ß√µes de datas conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "date_cols = [\n",
    "    \"approvaldate_319D\",\"creationdate_885D\",\"dateactivated_425D\",\n",
    "    \"dtlastpmt_581D\",\"dtlastpmtallstes_3545839D\",\"employedfrom_700D\",\n",
    "    \"firstnonzeroinstldate_307D\",\n",
    "]\n",
    "\n",
    "schema = dict(applprev1_filtrado.dtypes)\n",
    "date_valid = [c for c in date_cols if schema.get(c) in (\"date\",\"timestamp\")]\n",
    "\n",
    "aggs_dates = []\n",
    "for c in date_valid:\n",
    "    aggs_dates.append(F.min(c).alias(f\"{c}_min\"))\n",
    "    aggs_dates.append(F.max(c).alias(f\"{c}_max\"))\n",
    "\n",
    "agg_applprev_1_dates = applprev1_filtrado.groupBy(\"case_id\").agg(*aggs_dates)\n",
    "print(\"‚úÖ Agrega√ß√µes de datas conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6536069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agrega√ß√µes de flags conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "flag_cols = [\n",
    "    \"sem_aprovacao_flag\",\"sem_ativacao_flag\",\"sem_pagamento_flag\",\n",
    "    \"sem_pagamento_total_flag\",\"sem_emprego_flag\",\"sem_parcela_flag\",\n",
    "    \"mainoccupationinc_null_flag\",\"byoccupationinc_null_flag\",\n",
    "    \"tem_revolving_flag\",\"limite_cartao_credito_flag\",\"sem_historico_credito_flag\"\n",
    "]\n",
    "\n",
    "schema = dict(applprev1_filtrado.dtypes)\n",
    "flag_valid = [c for c in flag_cols if schema.get(c) in (\"int\",\"bigint\",\"double\")]\n",
    "\n",
    "aggs_flags = (\n",
    "    [F.max(F.col(c)).alias(f\"{c}_max\") for c in flag_valid] +   # se algum =1, fica 1\n",
    "    [F.sum(F.col(c)).alias(f\"{c}_sum\") for c in flag_valid]     # quantos registros marcaram 1\n",
    ")\n",
    "\n",
    "agg_applprev_1_flags = applprev1_filtrado.groupBy(\"case_id\").agg(*aggs_flags)\n",
    "print(\"‚úÖ Agrega√ß√µes de flags conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c7318f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agrega√ß√µes das colunas de dias conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "dias_cols = [\n",
    "    \"dias_para_aprovacao\",\"dias_ate_ativacao\",\"dias_ult_pagamento\",\n",
    "    \"dias_ult_pagamento_all\",\"dias_desde_inicio_emprego\",\"dias_para_primeira_parcela\"\n",
    "]\n",
    "\n",
    "schema = dict(applprev1_filtrado.dtypes)\n",
    "dias_valid = [c for c in dias_cols if schema.get(c) in (\"int\",\"bigint\",\"double\",\"float\",\"decimal\")]\n",
    "\n",
    "aggs_dias = (\n",
    "    [F.min(c).alias(f\"{c}_min\") for c in dias_valid] +\n",
    "    [F.avg(c).alias(f\"{c}_avg\") for c in dias_valid] +\n",
    "    [F.max(c).alias(f\"{c}_max\") for c in dias_valid]\n",
    ")\n",
    "\n",
    "agg_applprev_1_dias = applprev1_filtrado.groupBy(\"case_id\").agg(*aggs_dias)\n",
    "print(\"‚úÖ Agrega√ß√µes das colunas de dias conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14d24217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agrega√ß√µes categ√≥ricas conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "cat_cols = [\n",
    "    \"cancelreason_3545846M\",\"credacc_status_367L\",\"credtype_587L\",\n",
    "    \"district_544M\",\"education_1138M\",\"familystate_726L\",\"inittransactioncode_279L\",\n",
    "    \"postype_4733339M\",\"profession_152M\",\"rejectreason_755M\",\n",
    "    \"rejectreasonclient_4145042M\",\"status_219L\"\n",
    "]\n",
    "\n",
    "schema = dict(applprev1_filtrado.dtypes)\n",
    "cat_valid = [c for c in cat_cols if schema.get(c) in (\"string\",)]\n",
    "\n",
    "aggs_cat = (\n",
    "    [F.countDistinct(c).alias(f\"{c}_ndistinct\") for c in cat_valid] +\n",
    "    [F.first(c, ignorenulls=True).alias(f\"{c}_first\") for c in cat_valid]\n",
    ")\n",
    "\n",
    "agg_applprev_1_cat = applprev1_filtrado.groupBy(\"case_id\").agg(*aggs_cat)\n",
    "print(\"‚úÖ Agrega√ß√µes categ√≥ricas conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbfd463a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset agregado (train_applprev_1) pronto. Colunas: 139\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "to_join = [agg_applprev_1_num, agg_applprev_1_dates, agg_applprev_1_flags, agg_applprev_1_dias, agg_applprev_1_cat]\n",
    "to_join = [df for df in to_join if len(df.columns) > 1]  # mant√©m apenas os que t√™m m√©tricas\n",
    "\n",
    "agg_applprev_1_all = reduce(lambda l, r: l.join(r, on=\"case_id\", how=\"left\"), to_join)\n",
    "\n",
    "print(f\"‚úÖ Dataset agregado (train_applprev_1) pronto. Colunas: {len(agg_applprev_1_all.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37f55c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Validando DF agregado: colunas=139\n",
      "ü™™ Linhas: 1171847 | case_id distintos: 1171847\n",
      "‚ö†Ô∏è Colunas agregadas esperadas ausentes (ok se algumas categorias n√£o foram geradas):\n",
      "  - credacc_status_367L_ndistinct\n",
      "  - credacc_status_367L_first\n",
      "  - status_219L_ndistinct\n",
      "  - status_219L_first\n",
      "‚úÖ Valida√ß√£o conclu√≠da: nenhuma inconsist√™ncia encontrada nas m√©tricas agregadas.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --------------------------\n",
    "# 0) Refer√™ncias\n",
    "# --------------------------\n",
    "agg_df = agg_applprev_1_all  # DF final agregado por case_id\n",
    "print(f\"üîé Validando DF agregado: colunas={len(agg_df.columns)}\")\n",
    "\n",
    "# --------------------------\n",
    "# 1) Unicidade de case_id\n",
    "# --------------------------\n",
    "total = agg_df.count()\n",
    "distinct_ids = agg_df.select(\"case_id\").distinct().count()\n",
    "print(f\"ü™™ Linhas: {total} | case_id distintos: {distinct_ids}\")\n",
    "assert total == distinct_ids, \"‚ùå case_id n√£o √© √∫nico no agregado!\"\n",
    "\n",
    "# --------------------------\n",
    "# 2) Listas de colunas por categoria (como usadas na agrega√ß√£o)\n",
    "# --------------------------\n",
    "num_cols = [\n",
    "    \"annuity_853A\",\"credacc_actualbalance_314A\",\"credacc_credlmt_575A\",\n",
    "    \"credacc_maxhisbal_375A\",\"credacc_minhisbal_90A\",\"credacc_transactions_402L\",\n",
    "    \"credamount_590A\",\"currdebt_94A\",\"downpmt_134A\",\"mainoccupationinc_437A\",\n",
    "    \"byoccupationinc_3656910L\",\"outstandingdebt_522A\",\"pmtnum_8L\",\n",
    "    \"revolvingaccount_394A\",\"tenor_203L\",\"divida_total\"\n",
    "]\n",
    "\n",
    "date_cols = [\n",
    "    \"approvaldate_319D\",\"creationdate_885D\",\"dateactivated_425D\",\n",
    "    \"dtlastpmt_581D\",\"dtlastpmtallstes_3545839D\",\"employedfrom_700D\",\n",
    "    \"firstnonzeroinstldate_307D\",\n",
    "]\n",
    "\n",
    "flag_cols = [\n",
    "    \"sem_aprovacao_flag\",\"sem_ativacao_flag\",\"sem_pagamento_flag\",\n",
    "    \"sem_pagamento_total_flag\",\"sem_emprego_flag\",\"sem_parcela_flag\",\n",
    "    \"mainoccupationinc_null_flag\",\"byoccupationinc_null_flag\",\n",
    "    \"tem_revolving_flag\",\"limite_cartao_credito_flag\",\"sem_historico_credito_flag\"\n",
    "]\n",
    "\n",
    "dias_cols = [\n",
    "    \"dias_para_aprovacao\",\"dias_ate_ativacao\",\"dias_ult_pagamento\",\n",
    "    \"dias_ult_pagamento_all\",\"dias_desde_inicio_emprego\",\"dias_para_primeira_parcela\"\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"cancelreason_3545846M\",\"credacc_status_367L\",\"credtype_587L\",\n",
    "    \"district_544M\",\"education_1138M\",\"familystate_726L\",\"inittransactioncode_279L\",\n",
    "    \"postype_4733339M\",\"profession_152M\",\"rejectreason_755M\",\n",
    "    \"rejectreasonclient_4145042M\",\"status_219L\"\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# 3) Presen√ßa das colunas esperadas no DF agregado\n",
    "# --------------------------\n",
    "present = set(agg_df.columns)\n",
    "\n",
    "def _missing(expected_suffixes):\n",
    "    return [c for c in expected_suffixes if c not in present]\n",
    "\n",
    "expected_num = []\n",
    "for c in num_cols:\n",
    "    expected_num += [f\"{c}_sum\", f\"{c}_avg\", f\"{c}_max\", f\"{c}_min\"]\n",
    "\n",
    "expected_dates = []\n",
    "for c in date_cols:\n",
    "    expected_dates += [f\"{c}_min\", f\"{c}_max\"]\n",
    "\n",
    "expected_flags = []\n",
    "for c in flag_cols:\n",
    "    expected_flags += [f\"{c}_max\", f\"{c}_sum\"]\n",
    "\n",
    "expected_dias = []\n",
    "for c in dias_cols:\n",
    "    expected_dias += [f\"{c}_min\", f\"{c}_avg\", f\"{c}_max\"]\n",
    "\n",
    "expected_cat = []\n",
    "for c in cat_cols:\n",
    "    expected_cat += [f\"{c}_ndistinct\", f\"{c}_first\"]\n",
    "\n",
    "missing_any = (\n",
    "    _missing(expected_num) +\n",
    "    _missing(expected_dates) +\n",
    "    _missing(expected_flags) +\n",
    "    _missing(expected_dias) +\n",
    "    _missing(expected_cat)\n",
    ")\n",
    "\n",
    "if missing_any:\n",
    "    print(\"‚ö†Ô∏è Colunas agregadas esperadas ausentes (ok se algumas categorias n√£o foram geradas):\")\n",
    "    for m in missing_any[:50]:\n",
    "        print(\"  -\", m)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Regras de coer√™ncia das m√©tricas\n",
    "# --------------------------\n",
    "issues = {}\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "EPS = 1e-6  # toler√¢ncia num√©rica\n",
    "\n",
    "# 4.1 Num√©ricos: apenas checar se avg est√° entre min e max (com toler√¢ncia)\n",
    "for c in num_cols:\n",
    "    c_sum, c_avg, c_min, c_max = f\"{c}_sum\", f\"{c}_avg\", f\"{c}_min\", f\"{c}_max\"\n",
    "    if {c_sum, c_avg, c_min, c_max}.issubset(set(agg_df.columns)):\n",
    "        bad = (\n",
    "            agg_df\n",
    "            .filter(\n",
    "                F.col(c_min).isNotNull() & F.col(c_max).isNotNull() & F.col(c_avg).isNotNull()\n",
    "                & (\n",
    "                    (F.col(c_avg) + F.lit(EPS) < F.col(c_min)) |   # avg < min (com folga)\n",
    "                    (F.col(c_avg) - F.lit(EPS) > F.col(c_max))     # avg > max (com folga)\n",
    "                )\n",
    "            )\n",
    "            .select(\"case_id\", c_min, c_avg, c_max, c_sum)\n",
    "        )\n",
    "        if bad.limit(1).count() > 0:\n",
    "            issues[f\"num_metric_incoherent::{c}\"] = bad.limit(20)\n",
    "\n",
    "\n",
    "# 4.2 Datas: min ‚â§ max\n",
    "for c in date_cols:\n",
    "    c_min, c_max = f\"{c}_min\", f\"{c}_max\"\n",
    "    if {c_min, c_max}.issubset(present):\n",
    "        bad = (\n",
    "            agg_df\n",
    "            .filter(F.col(c_min).isNotNull() & F.col(c_max).isNotNull() & (F.col(c_min) > F.col(c_max)))\n",
    "            .select(\"case_id\", c_min, c_max)\n",
    "        )\n",
    "        if bad.limit(1).count() > 0:\n",
    "            issues[f\"date_min_gt_max::{c}\"] = bad.limit(20)\n",
    "\n",
    "# 4.3 Flags: max ‚àà {0,1} e sum ‚â• max\n",
    "for c in flag_cols:\n",
    "    c_max, c_sum = f\"{c}_max\", f\"{c}_sum\"\n",
    "    if {c_max, c_sum}.issubset(present):\n",
    "        bad = (\n",
    "            agg_df\n",
    "            .filter(\n",
    "                F.col(c_max).isNotNull() & (~F.col(c_max).isin(0,1))   # max deve ser 0/1\n",
    "                | (F.col(c_sum) < F.col(c_max))                        # sum n√£o pode ser < max\n",
    "            )\n",
    "            .select(\"case_id\", c_max, c_sum)\n",
    "        )\n",
    "        if bad.limit(1).count() > 0:\n",
    "            issues[f\"flag_incoherent::{c}\"] = bad.limit(20)\n",
    "\n",
    "# 4.4 dias_*: min ‚â§ avg ‚â§ max\n",
    "for c in dias_cols:\n",
    "    c_min, c_avg, c_max = f\"{c}_min\", f\"{c}_avg\", f\"{c}_max\"\n",
    "    if {c_min, c_avg, c_max}.issubset(present):\n",
    "        bad = (\n",
    "            agg_df\n",
    "            .filter(\n",
    "                (F.col(c_min).isNotNull()) & (F.col(c_avg).isNotNull()) & (F.col(c_max).isNotNull()) &\n",
    "                ((F.col(c_avg) < F.col(c_min)) | (F.col(c_avg) > F.col(c_max)))\n",
    "            )\n",
    "            .select(\"case_id\", c_min, c_avg, c_max)\n",
    "        )\n",
    "        if bad.limit(1).count() > 0:\n",
    "            issues[f\"dias_incoherent::{c}\"] = bad.limit(20)\n",
    "\n",
    "# --------------------------\n",
    "# 5) Relato de inconsist√™ncias\n",
    "# --------------------------\n",
    "if not issues:\n",
    "    print(\"‚úÖ Valida√ß√£o conclu√≠da: nenhuma inconsist√™ncia encontrada nas m√©tricas agregadas.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Inconsist√™ncias encontradas ({len(issues)} tipos). Exemplos por tipo:\")\n",
    "    for k, df_bad in issues.items():\n",
    "        print(f\"\\n‚îÄ‚îÄ {k} ‚îÄ‚îÄ\")\n",
    "        df_bad.show(truncate=False)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d01faabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case_id',\n",
       " 'annualeffectiverate_199L',\n",
       " 'annualeffectiverate_63L',\n",
       " 'classificationofcontr_13M',\n",
       " 'classificationofcontr_400M',\n",
       " 'contractst_545M',\n",
       " 'contractst_964M',\n",
       " 'contractsum_5085717L',\n",
       " 'credlmt_230A',\n",
       " 'credlmt_935A',\n",
       " 'dateofcredend_289D',\n",
       " 'dateofcredend_353D',\n",
       " 'dateofcredstart_181D',\n",
       " 'dateofcredstart_739D',\n",
       " 'dateofrealrepmt_138D',\n",
       " 'debtoutstand_525A',\n",
       " 'debtoverdue_47A',\n",
       " 'description_351M',\n",
       " 'dpdmax_139P',\n",
       " 'dpdmax_757P',\n",
       " 'dpdmaxdatemonth_442T',\n",
       " 'dpdmaxdatemonth_89T',\n",
       " 'dpdmaxdateyear_596T',\n",
       " 'dpdmaxdateyear_896T',\n",
       " 'financialinstitution_382M',\n",
       " 'financialinstitution_591M',\n",
       " 'instlamount_768A',\n",
       " 'instlamount_852A',\n",
       " 'interestrate_508L',\n",
       " 'lastupdate_1112D',\n",
       " 'lastupdate_388D',\n",
       " 'monthlyinstlamount_332A',\n",
       " 'monthlyinstlamount_674A',\n",
       " 'nominalrate_281L',\n",
       " 'nominalrate_498L',\n",
       " 'num_group1',\n",
       " 'numberofcontrsvalue_258L',\n",
       " 'numberofcontrsvalue_358L',\n",
       " 'numberofinstls_229L',\n",
       " 'numberofinstls_320L',\n",
       " 'numberofoutstandinstls_520L',\n",
       " 'numberofoutstandinstls_59L',\n",
       " 'numberofoverdueinstlmax_1039L',\n",
       " 'numberofoverdueinstlmax_1151L',\n",
       " 'numberofoverdueinstlmaxdat_148D',\n",
       " 'numberofoverdueinstlmaxdat_641D',\n",
       " 'numberofoverdueinstls_725L',\n",
       " 'numberofoverdueinstls_834L',\n",
       " 'outstandingamount_354A',\n",
       " 'outstandingamount_362A',\n",
       " 'overdueamount_31A',\n",
       " 'overdueamount_659A',\n",
       " 'overdueamountmax2_14A',\n",
       " 'overdueamountmax2_398A',\n",
       " 'overdueamountmax2date_1002D',\n",
       " 'overdueamountmax2date_1142D',\n",
       " 'overdueamountmax_155A',\n",
       " 'overdueamountmax_35A',\n",
       " 'overdueamountmaxdatemonth_284T',\n",
       " 'overdueamountmaxdatemonth_365T',\n",
       " 'overdueamountmaxdateyear_2T',\n",
       " 'overdueamountmaxdateyear_994T',\n",
       " 'periodicityofpmts_1102L',\n",
       " 'periodicityofpmts_837L',\n",
       " 'prolongationcount_1120L',\n",
       " 'prolongationcount_599L',\n",
       " 'purposeofcred_426M',\n",
       " 'purposeofcred_874M',\n",
       " 'refreshdate_3813885D',\n",
       " 'residualamount_488A',\n",
       " 'residualamount_856A',\n",
       " 'subjectrole_182M',\n",
       " 'subjectrole_93M',\n",
       " 'totalamount_6A',\n",
       " 'totalamount_996A',\n",
       " 'totaldebtoverduevalue_178A',\n",
       " 'totaldebtoverduevalue_718A',\n",
       " 'totaloutstanddebtvalue_39A',\n",
       " 'totaloutstanddebtvalue_668A',\n",
       " 'annualeffectiverate_199L_flag',\n",
       " 'annualeffectiverate_63L_flag',\n",
       " 'contractsum_5085717L_flag',\n",
       " 'credlmt_230A_flag',\n",
       " 'credlmt_935A_flag',\n",
       " 'dateofcredend_289D_flag',\n",
       " 'dateofcredend_353D_flag',\n",
       " 'dateofcredstart_181D_flag',\n",
       " 'dateofcredstart_739D_flag',\n",
       " 'dateofrealrepmt_138D_flag',\n",
       " 'debtoutstand_525A_flag',\n",
       " 'debtoverdue_47A_flag',\n",
       " 'dpdmax_139P_flag',\n",
       " 'dpdmax_757P_flag',\n",
       " 'dpdmaxdatemonth_442T_flag',\n",
       " 'dpdmaxdatemonth_89T_flag',\n",
       " 'dpdmaxdateyear_596T_flag',\n",
       " 'dpdmaxdateyear_896T_flag',\n",
       " 'instlamount_768A_flag',\n",
       " 'instlamount_852A_flag',\n",
       " 'interestrate_508L_flag',\n",
       " 'lastupdate_1112D_flag',\n",
       " 'lastupdate_388D_flag',\n",
       " 'monthlyinstlamount_332A_flag',\n",
       " 'monthlyinstlamount_674A_flag',\n",
       " 'nominalrate_281L_flag',\n",
       " 'nominalrate_498L_flag',\n",
       " 'numberofcontrsvalue_258L_flag',\n",
       " 'numberofcontrsvalue_358L_flag',\n",
       " 'numberofinstls_229L_flag',\n",
       " 'numberofinstls_320L_flag',\n",
       " 'numberofoutstandinstls_520L_flag',\n",
       " 'numberofoutstandinstls_59L_flag',\n",
       " 'numberofoverdueinstlmax_1039L_flag',\n",
       " 'numberofoverdueinstlmax_1151L_flag',\n",
       " 'numberofoverdueinstlmaxdat_148D_flag',\n",
       " 'numberofoverdueinstlmaxdat_641D_flag',\n",
       " 'numberofoverdueinstls_725L_flag',\n",
       " 'numberofoverdueinstls_834L_flag',\n",
       " 'outstandingamount_354A_flag',\n",
       " 'outstandingamount_362A_flag',\n",
       " 'overdueamount_31A_flag',\n",
       " 'overdueamount_659A_flag',\n",
       " 'overdueamountmax2_14A_flag',\n",
       " 'overdueamountmax2_398A_flag',\n",
       " 'overdueamountmax2date_1002D_flag',\n",
       " 'overdueamountmax2date_1142D_flag',\n",
       " 'overdueamountmax_155A_flag',\n",
       " 'overdueamountmax_35A_flag',\n",
       " 'overdueamountmaxdatemonth_284T_flag',\n",
       " 'overdueamountmaxdatemonth_365T_flag',\n",
       " 'overdueamountmaxdateyear_2T_flag',\n",
       " 'overdueamountmaxdateyear_994T_flag',\n",
       " 'periodicityofpmts_1102L_flag',\n",
       " 'periodicityofpmts_837L_flag',\n",
       " 'prolongationcount_1120L_flag',\n",
       " 'prolongationcount_599L_flag',\n",
       " 'refreshdate_3813885D_flag',\n",
       " 'residualamount_488A_flag',\n",
       " 'residualamount_856A_flag',\n",
       " 'totalamount_6A_flag',\n",
       " 'totalamount_996A_flag',\n",
       " 'totaldebtoverduevalue_178A_flag',\n",
       " 'totaldebtoverduevalue_718A_flag',\n",
       " 'totaloutstanddebtvalue_39A_flag',\n",
       " 'totaloutstanddebtvalue_668A_flag']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_a1_filtrado.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa579d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Num√©ricas v√°lidas: 48\n",
      "‚úÖ Agrega√ß√µes num√©ricas conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = bureau_a1_filtrado\n",
    "\n",
    "schema = dict(df.dtypes)\n",
    "\n",
    "# Colunas num√©ricas (exclui flags, datas D, partes de data T e categ√≥ricas M)\n",
    "num_candidates = [\n",
    "    c for c in df.columns\n",
    "    if not c.endswith(\"_flag\")\n",
    "    and not c.endswith(\"D\")  # datas\n",
    "    and not c.endswith(\"T\")  # partes de data (m√™s/ano) - trataremos √† parte\n",
    "    and not c.endswith(\"M\")  # categ√≥ricas\n",
    "]\n",
    "\n",
    "num_valid = [c for c in num_candidates if schema.get(c) in (\"int\",\"bigint\",\"double\",\"float\",\"decimal\")]\n",
    "\n",
    "print(\"üìä Num√©ricas v√°lidas:\", len(num_valid))\n",
    "\n",
    "aggs_num = (\n",
    "    [F.sum(c).alias(f\"{c}_sum\") for c in num_valid] +\n",
    "    [F.avg(c).alias(f\"{c}_avg\") for c in num_valid] +\n",
    "    [F.max(c).alias(f\"{c}_max\") for c in num_valid] +\n",
    "    [F.min(c).alias(f\"{c}_min\") for c in num_valid]\n",
    ")\n",
    "\n",
    "agg_bureau_a1_num = df.groupBy(\"case_id\").agg(*aggs_num)\n",
    "print(\"‚úÖ Agrega√ß√µes num√©ricas conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ae36c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Datas v√°lidas: 12\n",
      "‚úÖ Agrega√ß√µes de datas conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "# Datas: termina com D e tipo date/timestamp\n",
    "date_candidates = [c for c in df.columns if c.endswith(\"D\")]\n",
    "date_valid = [c for c in date_candidates if schema.get(c) in (\"date\",\"timestamp\")]\n",
    "\n",
    "print(\"üìÖ Datas v√°lidas:\", len(date_valid))\n",
    "\n",
    "aggs_dates = []\n",
    "for c in date_valid:\n",
    "    aggs_dates.append(F.min(c).alias(f\"{c}_min\"))\n",
    "    aggs_dates.append(F.max(c).alias(f\"{c}_max\"))\n",
    "\n",
    "agg_bureau_a1_dates = df.groupBy(\"case_id\").agg(*aggs_dates) if aggs_dates else None\n",
    "print(\"‚úÖ Agrega√ß√µes de datas conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acec548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö© Flags v√°lidas: 66\n",
      "‚úÖ Agrega√ß√µes de flags conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "flag_cols = [c for c in df.columns if c.endswith(\"_flag\")]\n",
    "flag_valid = [c for c in flag_cols if schema.get(c) in (\"int\",\"bigint\",\"double\")]\n",
    "\n",
    "print(\"üö© Flags v√°lidas:\", len(flag_valid))\n",
    "\n",
    "aggs_flags = (\n",
    "    [F.max(F.col(c)).alias(f\"{c}_max\") for c in flag_valid] +  # se algum registro = 1 ‚Üí 1\n",
    "    [F.sum(F.col(c)).alias(f\"{c}_sum\") for c in flag_valid]    # quantos registros com 1\n",
    ")\n",
    "\n",
    "agg_bureau_a1_flags = df.groupBy(\"case_id\").agg(*aggs_flags) if aggs_flags else None\n",
    "print(\"‚úÖ Agrega√ß√µes de flags conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01b4bead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóìÔ∏è Partes de data v√°lidas (T): 8\n",
      "‚úÖ Agrega√ß√µes de partes de data conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "# Partes de data (T) ‚Äî geralmente m√™s (1‚Äì12) e ano (YYYY)\n",
    "t_candidates = [c for c in df.columns if c.endswith(\"T\")]\n",
    "t_valid = [c for c in t_candidates if schema.get(c) in (\"int\",\"bigint\",\"double\",\"float\",\"decimal\")]\n",
    "\n",
    "print(\"üóìÔ∏è Partes de data v√°lidas (T):\", len(t_valid))\n",
    "\n",
    "aggs_t = []\n",
    "for c in t_valid:\n",
    "    aggs_t.append(F.min(c).alias(f\"{c}_min\"))\n",
    "    aggs_t.append(F.max(c).alias(f\"{c}_max\"))\n",
    "\n",
    "agg_bureau_a1_t = df.groupBy(\"case_id\").agg(*aggs_t) if aggs_t else None\n",
    "print(\"‚úÖ Agrega√ß√µes de partes de data conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebe8b8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Categ√≥ricas v√°lidas: 11\n",
      "‚úÖ Agrega√ß√µes categ√≥ricas conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "# Categ√≥ricas: termina com M ou tipo string\n",
    "cat_candidates = list({c for c in df.columns if c.endswith(\"M\")} | {c for c,t in schema.items() if t == \"string\"})\n",
    "cat_valid = [c for c in cat_candidates if schema.get(c) == \"string\"]\n",
    "\n",
    "print(\"üî§ Categ√≥ricas v√°lidas:\", len(cat_valid))\n",
    "\n",
    "aggs_cat = (\n",
    "    [F.countDistinct(c).alias(f\"{c}_ndistinct\") for c in cat_valid] +\n",
    "    [F.first(c, ignorenulls=True).alias(f\"{c}_first\") for c in cat_valid]\n",
    ")\n",
    "\n",
    "agg_bureau_a1_cat = df.groupBy(\"case_id\").agg(*aggs_cat) if aggs_cat else None\n",
    "print(\"‚úÖ Agrega√ß√µes categ√≥ricas conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70d1d7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset agregado (train_credit_bureau_a_1) pronto. Colunas: 387\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "to_join = [agg_bureau_a1_num, agg_bureau_a1_dates, agg_bureau_a1_flags, agg_bureau_a1_t, agg_bureau_a1_cat]\n",
    "to_join = [d for d in to_join if d is not None and len(d.columns) > 1]\n",
    "\n",
    "agg_credit_bureau_a_1_all = reduce(lambda l, r: l.join(r, on=\"case_id\", how=\"left\"), to_join)\n",
    "\n",
    "print(f\"‚úÖ Dataset agregado (train_credit_bureau_a_1) pronto. Colunas: {len(agg_credit_bureau_a_1_all.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f3969b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Validando DF agregado: colunas=387\n",
      "ü™™ Linhas: 1386248 | case_id distintos: 1386248\n",
      "‚úÖ Valida√ß√£o conclu√≠da: nenhuma inconsist√™ncia encontrada.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = agg_credit_bureau_a_1_all\n",
    "EPS = 1e-6\n",
    "ANO_MIN, ANO_MAX = 1950, 2025\n",
    "\n",
    "print(f\"üîé Validando DF agregado: colunas={len(df.columns)}\")\n",
    "\n",
    "# 1) Unicidade de case_id\n",
    "total = df.count()\n",
    "distinct_ids = df.select(\"case_id\").distinct().count()\n",
    "print(f\"ü™™ Linhas: {total} | case_id distintos: {distinct_ids}\")\n",
    "assert total == distinct_ids, \"‚ùå case_id n√£o √© √∫nico no agregado!\"\n",
    "\n",
    "cols = set(df.columns)\n",
    "\n",
    "# 2) Descobrir fam√≠lias de colunas agregadas por sufixo\n",
    "def fam(prefixes):\n",
    "    return [c for c in df.columns if any(c.endswith(suf) for suf in prefixes)]\n",
    "\n",
    "num_min   = fam([\"_min\"])\n",
    "num_max   = fam([\"_max\"])\n",
    "num_avg   = fam([\"_avg\"])\n",
    "num_sum   = fam([\"_sum\"])\n",
    "\n",
    "# Datas: terminam com _min/_max mas eram de colunas 'D' no original\n",
    "date_min  = [c for c in num_min if \"_D_\" in c or c.endswith(\"D_min\")]\n",
    "date_max  = [c for c in num_max if \"_D_\" in c or c.endswith(\"D_max\")]\n",
    "\n",
    "# Flags: agregadas com _max e _sum e terminam com \"_flag_...\"\n",
    "flag_max  = [c for c in num_max if \"_flag_\" in c]\n",
    "flag_sum  = [c for c in num_sum if \"_flag_\" in c]\n",
    "\n",
    "# Partes de data (T): agregadas com _min/_max e tinham 'T' no nome original\n",
    "t_min     = [c for c in num_min if \"_T_\" in c or c.endswith(\"T_min\")]\n",
    "t_max     = [c for c in num_max if \"_T_\" in c or c.endswith(\"T_max\")]\n",
    "\n",
    "issues = {}\n",
    "\n",
    "# 3) NUM√âRICOS: avg dentro de [min, max] (com toler√¢ncia)\n",
    "#    Procuramos pares baseando-nos no prefixo do nome (antes do sufixo _min/_avg/_max/_sum)\n",
    "def base_name(colname, suf):\n",
    "    assert colname.endswith(suf)\n",
    "    return colname[: -len(suf)]\n",
    "\n",
    "bases = set()\n",
    "for c in (num_min + num_avg + num_max):\n",
    "    if c.endswith(\"_min\") or c.endswith(\"_avg\") or c.endswith(\"_max\"):\n",
    "        for suf in (\"_min\", \"_avg\", \"_max\"):\n",
    "            if c.endswith(suf):\n",
    "                bases.add(base_name(c, suf))\n",
    "\n",
    "for b in bases:\n",
    "    c_min, c_avg, c_max = f\"{b}_min\", f\"{b}_avg\", f\"{b}_max\"\n",
    "    if {c_min, c_avg, c_max}.issubset(cols):\n",
    "        bad = (\n",
    "            df.filter(\n",
    "                F.col(c_min).isNotNull() & F.col(c_avg).isNotNull() & F.col(c_max).isNotNull() &\n",
    "                (\n",
    "                    (F.col(c_avg) + F.lit(EPS) < F.col(c_min)) |\n",
    "                    (F.col(c_avg) - F.lit(EPS) > F.col(c_max))\n",
    "                )\n",
    "            )\n",
    "            .select(\"case_id\", c_min, c_avg, c_max)\n",
    "        )\n",
    "        if bad.limit(1).count() > 0:\n",
    "            issues[f\"num_avg_out_of_range::{b}\"] = bad.limit(20)\n",
    "\n",
    "# 4) DATAS: min ‚â§ max\n",
    "#    Detecta pares *_min / *_max que s√£o de datas (D)\n",
    "date_bases = set()\n",
    "for c in (date_min + date_max):\n",
    "    for suf in (\"_min\", \"_max\"):\n",
    "        if c.endswith(suf):\n",
    "            date_bases.add(base_name(c, suf))\n",
    "\n",
    "for b in date_bases:\n",
    "    cmin, cmax = f\"{b}_min\", f\"{b}_max\"\n",
    "    if {cmin, cmax}.issubset(cols):\n",
    "        bad = df.filter(F.col(cmin).isNotNull() & F.col(cmax).isNotNull() & (F.col(cmin) > F.col(cmax))) \\\n",
    "                .select(\"case_id\", cmin, cmax)\n",
    "        if bad.limit(1).count() > 0:\n",
    "            issues[f\"date_min_gt_max::{b}\"] = bad.limit(20)\n",
    "\n",
    "# 5) FLAGS: max ‚àà {0,1} e sum ‚â• max\n",
    "for cmax in flag_max:\n",
    "    b = base_name(cmax, \"_max\")\n",
    "    csum = f\"{b}_sum\"\n",
    "    if csum in cols:\n",
    "        bad = (\n",
    "            df.filter(\n",
    "                (F.col(cmax).isNotNull() & (~F.col(cmax).isin(0,1))) |\n",
    "                (F.col(csum) < F.col(cmax))\n",
    "            )\n",
    "            .select(\"case_id\", cmax, csum)\n",
    "        )\n",
    "        if bad.limit(1).count() > 0:\n",
    "            issues[f\"flag_incoherent::{b}\"] = bad.limit(20)\n",
    "\n",
    "# 6) PARTES DE DATA (T):\n",
    "#    Regras de faixa por nome:\n",
    "#    - *datemonth*  ‚Üí 1..12\n",
    "#    - *dateyear*   ‚Üí ANO_MIN..ANO_MAX\n",
    "for cmin in t_min:\n",
    "    b = base_name(cmin, \"_min\")\n",
    "    cmax = f\"{b}_max\"\n",
    "    if cmax in cols:\n",
    "        # faixa por padr√£o: apenas min<=max\n",
    "        bad_order = df.filter(F.col(cmin).isNotNull() & F.col(cmax).isNotNull() & (F.col(cmin) > F.col(cmax))) \\\n",
    "                    .select(\"case_id\", cmin, cmax)\n",
    "        if bad_order.limit(1).count() > 0:\n",
    "            issues[f\"T_min_gt_max::{b}\"] = bad_order.limit(20)\n",
    "\n",
    "        # se o nome contiver \"datemonth\": 1..12\n",
    "        if \"datemonth\" in b.lower():\n",
    "            bad_month = df.filter(\n",
    "                (F.col(cmin).isNotNull() & (F.col(cmin) < 1)) | (F.col(cmin) > 12) |\n",
    "                (F.col(cmax).isNotNull() & (F.col(cmax) < 1)) | (F.col(cmax) > 12)\n",
    "            ).select(\"case_id\", cmin, cmax)\n",
    "            if bad_month.limit(1).count() > 0:\n",
    "                issues[f\"T_month_out_of_range::{b}\"] = bad_month.limit(20)\n",
    "\n",
    "        # se o nome contiver \"dateyear\": ANO_MIN..ANO_MAX\n",
    "        if \"dateyear\" in b.lower():\n",
    "            bad_year = df.filter(\n",
    "                (F.col(cmin).isNotNull() & (F.col(cmin) < ANO_MIN)) | (F.col(cmin) > ANO_MAX) |\n",
    "                (F.col(cmax).isNotNull() & (F.col(cmax) < ANO_MIN)) | (F.col(cmax) > ANO_MAX)\n",
    "            ).select(\"case_id\", cmin, cmax)\n",
    "            if bad_year.limit(1).count() > 0:\n",
    "                issues[f\"T_year_out_of_range::{b}\"] = bad_year.limit(20)\n",
    "\n",
    "# 7) Relato\n",
    "if not issues:\n",
    "    print(\"‚úÖ Valida√ß√£o conclu√≠da: nenhuma inconsist√™ncia encontrada.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Inconsist√™ncias encontradas ({len(issues)} tipos). Exemplos por tipo:\")\n",
    "    for name, sample in issues.items():\n",
    "        print(f\"\\n‚îÄ‚îÄ {name} ‚îÄ‚îÄ\")\n",
    "        sample.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db035fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case_id',\n",
       " 'collater_typofvalofguarant_298M',\n",
       " 'collater_typofvalofguarant_407M',\n",
       " 'collater_valueofguarantee_1124L',\n",
       " 'collater_valueofguarantee_876L',\n",
       " 'collaterals_typeofguarante_359M',\n",
       " 'collaterals_typeofguarante_669M',\n",
       " 'num_group1',\n",
       " 'num_group2',\n",
       " 'pmts_dpd_1073P',\n",
       " 'pmts_dpd_303P',\n",
       " 'pmts_month_158T',\n",
       " 'pmts_month_706T',\n",
       " 'pmts_overdue_1140A',\n",
       " 'pmts_overdue_1152A',\n",
       " 'pmts_year_1139T',\n",
       " 'pmts_year_507T',\n",
       " 'subjectroles_name_541M',\n",
       " 'subjectroles_name_838M',\n",
       " 'collater_valueofguarantee_1124L_flag',\n",
       " 'collater_valueofguarantee_876L_flag',\n",
       " 'pmts_dpd_1073P_flag',\n",
       " 'pmts_dpd_303P_flag',\n",
       " 'pmts_month_158T_flag',\n",
       " 'pmts_month_706T_flag',\n",
       " 'pmts_overdue_1140A_flag',\n",
       " 'pmts_overdue_1152A_flag',\n",
       " 'pmts_year_1139T_flag',\n",
       " 'pmts_year_507T_flag']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_a2_filtrado.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07451272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Num√©ricas v√°lidas: 9\n",
      "‚úÖ Agrega√ß√µes num√©ricas conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = bureau_a2_filtrado\n",
    "schema = dict(df.dtypes)\n",
    "\n",
    "# Candidatas num√©ricas (exclui flags e categ√≥ricas M e partes de data T)\n",
    "num_candidates = [\n",
    "    c for c in df.columns\n",
    "    if not c.endswith(\"_flag\") and not c.endswith(\"M\") and not c.endswith(\"T\")\n",
    "]\n",
    "num_valid = [c for c in num_candidates if schema.get(c) in (\"int\",\"bigint\",\"double\",\"float\",\"decimal\")]\n",
    "\n",
    "print(\"üìä Num√©ricas v√°lidas:\", len(num_valid))\n",
    "\n",
    "aggs_num = (\n",
    "    [F.sum(c).alias(f\"{c}_sum\") for c in num_valid] +\n",
    "    [F.avg(c).alias(f\"{c}_avg\") for c in num_valid] +\n",
    "    [F.max(c).alias(f\"{c}_max\") for c in num_valid] +\n",
    "    [F.min(c).alias(f\"{c}_min\") for c in num_valid]\n",
    ")\n",
    "\n",
    "agg_bureau_a2_num = df.groupBy(\"case_id\").agg(*aggs_num)\n",
    "print(\"‚úÖ Agrega√ß√µes num√©ricas conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4280c8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóìÔ∏è Partes de data (T) v√°lidas: 4\n",
      "‚úÖ Agrega√ß√µes de T conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "t_candidates = [c for c in df.columns if c.endswith(\"T\")]\n",
    "t_valid = [c for c in t_candidates if schema.get(c) in (\"int\",\"bigint\",\"double\",\"float\",\"decimal\")]\n",
    "\n",
    "print(\"üóìÔ∏è Partes de data (T) v√°lidas:\", len(t_valid))\n",
    "\n",
    "aggs_t = []\n",
    "for c in t_valid:\n",
    "    aggs_t.append(F.min(c).alias(f\"{c}_min\"))\n",
    "    aggs_t.append(F.max(c).alias(f\"{c}_max\"))\n",
    "\n",
    "agg_bureau_a2_t = df.groupBy(\"case_id\").agg(*aggs_t) if aggs_t else None\n",
    "print(\"‚úÖ Agrega√ß√µes de T conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a18b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö© Flags v√°lidas: 10\n",
      "‚úÖ Agrega√ß√µes de flags conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "flag_cols = [c for c in df.columns if c.endswith(\"_flag\")]\n",
    "flag_valid = [c for c in flag_cols if schema.get(c) in (\"int\",\"bigint\",\"double\")]\n",
    "\n",
    "print(\"üö© Flags v√°lidas:\", len(flag_valid))\n",
    "\n",
    "aggs_flags = (\n",
    "    [F.max(F.col(c)).alias(f\"{c}_max\") for c in flag_valid] +\n",
    "    [F.sum(F.col(c)).alias(f\"{c}_sum\") for c in flag_valid]\n",
    ")\n",
    "\n",
    "agg_bureau_a2_flags = df.groupBy(\"case_id\").agg(*aggs_flags) if aggs_flags else None\n",
    "print(\"‚úÖ Agrega√ß√µes de flags conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed236f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Categ√≥ricas v√°lidas: 6\n",
      "‚úÖ Agrega√ß√µes categ√≥ricas conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "cat_candidates = [c for c in df.columns if c.endswith(\"M\")]\n",
    "cat_valid = [c for c in cat_candidates if schema.get(c) == \"string\"]\n",
    "\n",
    "print(\"üî§ Categ√≥ricas v√°lidas:\", len(cat_valid))\n",
    "\n",
    "aggs_cat = (\n",
    "    [F.countDistinct(c).alias(f\"{c}_ndistinct\") for c in cat_valid] +\n",
    "    [F.first(c, ignorenulls=True).alias(f\"{c}_first\") for c in cat_valid]\n",
    ")\n",
    "\n",
    "agg_bureau_a2_cat = df.groupBy(\"case_id\").agg(*aggs_cat) if aggs_cat else None\n",
    "print(\"‚úÖ Agrega√ß√µes categ√≥ricas conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "321fe85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset agregado (train_bureau_a_2) pronto. Colunas: 77\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "to_join = [agg_bureau_a2_num, agg_bureau_a2_t, agg_bureau_a2_flags, agg_bureau_a2_cat]\n",
    "to_join = [d for d in to_join if d is not None and len(d.columns) > 1]\n",
    "\n",
    "agg_credit_bureau_a_2_all = reduce(lambda l, r: l.join(r, on=\"case_id\", how=\"left\"), to_join)\n",
    "\n",
    "print(f\"‚úÖ Dataset agregado (train_bureau_a_2) pronto. Colunas: {len(agg_credit_bureau_a_2_all.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6c52361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Validando DF agregado: colunas=77\n",
      "ü™™ Linhas: 1385288 | case_id distintos: 1385288\n",
      "‚ö†Ô∏è Inconsist√™ncias encontradas (1 tipos). Exemplos por tipo:\n",
      "\n",
      "‚îÄ‚îÄ T_year_out_of_range::pmts_year_507T ‚îÄ‚îÄ\n",
      "+-------+------------------+------------------+\n",
      "|case_id|pmts_year_507T_min|pmts_year_507T_max|\n",
      "+-------+------------------+------------------+\n",
      "|254453 |2005.0            |2028.0            |\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_agg = agg_credit_bureau_a_2_all\n",
    "EPS = 1e-6\n",
    "ANO_MIN, ANO_MAX = 1950, 2025\n",
    "\n",
    "print(f\"üîé Validando DF agregado: colunas={len(df_agg.columns)}\")\n",
    "\n",
    "# Unicidade\n",
    "n = df_agg.count()\n",
    "n_ids = df_agg.select(\"case_id\").distinct().count()\n",
    "print(f\"ü™™ Linhas: {n} | case_id distintos: {n_ids}\")\n",
    "assert n == n_ids, \"‚ùå case_id n√£o √© √∫nico no agregado!\"\n",
    "\n",
    "cols = set(df_agg.columns)\n",
    "\n",
    "# Fam√≠lias\n",
    "def fam(suf):\n",
    "    return [c for c in df_agg.columns if c.endswith(suf)]\n",
    "\n",
    "mins  = fam(\"_min\")\n",
    "maxs  = fam(\"_max\")\n",
    "avgs  = fam(\"_avg\")\n",
    "\n",
    "# (a) Coer√™ncia num√©rica: avg entre min e max (quando existirem)\n",
    "bases = set(c[:-4] for c in mins) | set(c[:-4] for c in maxs) | set(c[:-4] for c in avgs)\n",
    "issues = {}\n",
    "\n",
    "for b in bases:\n",
    "    cmin, cavg, cmax = f\"{b}_min\", f\"{b}_avg\", f\"{b}_max\"\n",
    "    if {cmin, cavg, cmax}.issubset(cols):\n",
    "        bad = df_agg.filter(\n",
    "            F.col(cmin).isNotNull() & F.col(cavg).isNotNull() & F.col(cmax).isNotNull() &\n",
    "            ((F.col(cavg) + F.lit(EPS) < F.col(cmin)) | (F.col(cavg) - F.lit(EPS) > F.col(cmax)))\n",
    "        ).select(\"case_id\", cmin, cavg, cmax)\n",
    "        if bad.limit(1).count() > 0:\n",
    "            issues[f\"avg_out_of_range::{b}\"] = bad.limit(20)\n",
    "\n",
    "# (b) Partes de data (T): faixa de m√™s/ano\n",
    "t_min = [c for c in mins if \"T_\" in c or c.endswith(\"T_min\")]\n",
    "for cmin in t_min:\n",
    "    b = cmin[:-4]\n",
    "    cmax = f\"{b}_max\"\n",
    "    if cmax in cols:\n",
    "        # ordem\n",
    "        bad_order = df_agg.filter(\n",
    "            F.col(cmin).isNotNull() & F.col(cmax).isNotNull() & (F.col(cmin) > F.col(cmax))\n",
    "        ).select(\"case_id\", cmin, cmax)\n",
    "        if bad_order.limit(1).count() > 0:\n",
    "            issues[f\"T_min_gt_max::{b}\"] = bad_order.limit(20)\n",
    "\n",
    "        # m√™s 1..12\n",
    "        if \"month\" in b.lower():\n",
    "            bad_month = df_agg.filter(\n",
    "                (F.col(cmin).isNotNull() & ((F.col(cmin) < 1) | (F.col(cmin) > 12))) |\n",
    "                (F.col(cmax).isNotNull() & ((F.col(cmax) < 1) | (F.col(cmax) > 12)))\n",
    "            ).select(\"case_id\", cmin, cmax)\n",
    "            if bad_month.limit(1).count() > 0:\n",
    "                issues[f\"T_month_out_of_range::{b}\"] = bad_month.limit(20)\n",
    "\n",
    "        # ano plaus√≠vel\n",
    "        if \"year\" in b.lower():\n",
    "            bad_year = df_agg.filter(\n",
    "                (F.col(cmin).isNotNull() & ((F.col(cmin) < ANO_MIN) | (F.col(cmin) > ANO_MAX))) |\n",
    "                (F.col(cmax).isNotNull() & ((F.col(cmax) < ANO_MIN) | (F.col(cmax) > ANO_MAX)))\n",
    "            ).select(\"case_id\", cmin, cmax)\n",
    "            if bad_year.limit(1).count() > 0:\n",
    "                issues[f\"T_year_out_of_range::{b}\"] = bad_year.limit(20)\n",
    "\n",
    "# (c) Flags: max ‚àà {0,1} e sum ‚â• max\n",
    "flag_max = [c for c in maxs if \"_flag_\" in c]\n",
    "for cmax in flag_max:\n",
    "    b = cmax[:-4]\n",
    "    csum = f\"{b}_sum\"\n",
    "    if csum in cols:\n",
    "        bad = df_agg.filter(\n",
    "            (F.col(cmax).isNotNull() & (~F.col(cmax).isin(0,1))) |\n",
    "            (F.col(csum) < F.col(cmax))\n",
    "        ).select(\"case_id\", cmax, csum)\n",
    "        if bad.limit(1).count() > 0:\n",
    "            issues[f\"flag_incoherent::{b}\"] = bad.limit(20)\n",
    "\n",
    "# Resultado\n",
    "if not issues:\n",
    "    print(\"‚úÖ Valida√ß√£o conclu√≠da: nenhuma inconsist√™ncia encontrada.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Inconsist√™ncias encontradas ({len(issues)} tipos). Exemplos por tipo:\")\n",
    "    for k, d in issues.items():\n",
    "        print(f\"\\n‚îÄ‚îÄ {k} ‚îÄ‚îÄ\")\n",
    "        d.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8820e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
